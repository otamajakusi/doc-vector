{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "176owOOFkPpsjaaMtl-dHko-L_ifAQYrQ",
      "authorship_tag": "ABX9TyN8xMHw2XffbIW0vVyGT4kj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/otamajakusi/doc-vector/blob/main/sbert-sts/sbert_sts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yL6S_9nD64TZ",
        "outputId": "5db55384-39df-4a5f-946c-225d298a9a3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'doc-vector'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
            "remote: Total 146 (delta 55), reused 112 (delta 34), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (146/146), 44.36 KiB | 11.09 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/otamajakusi/doc-vector"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd doc-vector/sbert-sts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7GLhRkb7In9",
        "outputId": "05de5613-ffb2-411d-8d7d-ce70521e16c6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/doc-vector/sbert-sts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGybO2W07SQN",
        "outputId": "8be37ce3-99e3-49ac-b8e0-76dca8d24f15"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fugashi\n",
            "  Downloading fugashi-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (599 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.9/599.9 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipadic\n",
            "  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unidic-lite\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.5.3)\n",
            "Collecting ja_ginza\n",
            "  Downloading ja_ginza-5.1.2-py3-none-any.whl (59.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (2.12.2)\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 1)) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 1)) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 1)) (0.15.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 1)) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 1)) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 1)) (3.8.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.98-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 5)) (2022.7.1)\n",
            "Collecting sudachipy<0.7.0,>=0.6.2\n",
            "  Downloading SudachiPy-0.6.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ginza<5.2.0,>=5.1.0\n",
            "  Downloading ginza-5.1.2-py3-none-any.whl (20 kB)\n",
            "Collecting spacy<3.5.0,>=3.2.0\n",
            "  Downloading spacy-3.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sudachidict-core>=20210802\n",
            "  Downloading SudachiDict-core-20230110.tar.gz (9.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (0.40.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.54.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (0.7.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (67.7.2)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.20.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (2.27.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (2.3.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.4.3)\n",
            "Collecting plac>=1.3.3\n",
            "  Downloading plac-1.3.5-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (1.16.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (0.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (2023.4.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (3.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 7)) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 7)) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 7)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 7)) (3.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.2.0->ja_ginza->-r requirements.txt (line 6)) (2.4.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.2.0->ja_ginza->-r requirements.txt (line 6)) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.2.0->ja_ginza->-r requirements.txt (line 6)) (8.1.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.2.0->ja_ginza->-r requirements.txt (line 6)) (3.1.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.2.0->ja_ginza->-r requirements.txt (line 6)) (3.0.12)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.2.0->ja_ginza->-r requirements.txt (line 6)) (1.0.9)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.2.0->ja_ginza->-r requirements.txt (line 6)) (3.3.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.2.0->ja_ginza->-r requirements.txt (line 6)) (6.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.2.0->ja_ginza->-r requirements.txt (line 6)) (2.0.7)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.2.0->ja_ginza->-r requirements.txt (line 6)) (1.10.7)\n",
            "Collecting wasabi<1.1.0,>=0.9.1\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.2.0->ja_ginza->-r requirements.txt (line 6)) (0.10.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.2.0->ja_ginza->-r requirements.txt (line 6)) (2.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.2.0->ja_ginza->-r requirements.txt (line 6)) (1.0.4)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.2.0->ja_ginza->-r requirements.txt (line 6)) (0.7.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers->-r requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers->-r requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers->-r requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers->-r requirements.txt (line 1)) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers->-r requirements.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->-r requirements.txt (line 1)) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m116.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 7)) (2.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers->-r requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers->-r requirements.txt (line 1)) (8.1.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers->-r requirements.txt (line 1)) (8.4.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 7)) (3.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.2.0->ja_ginza->-r requirements.txt (line 6)) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.2.0->ja_ginza->-r requirements.txt (line 6)) (0.0.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers->-r requirements.txt (line 1)) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers, ipadic, unidic-lite, sudachidict-core\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=c4e7100090686a35987a4d839a0c0c5a86504491932940a0702808b3b2da6b0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556721 sha256=9206186cd22d63a8defaf61d8092b725eec4e9eb68ee8dbeb4422b3103b506d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/ea/e3/2f6e0860a327daba3b030853fce4483ed37468bbf1101c59c3\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658834 sha256=ed4e3031c3c74a02c4d78ea66cc68e09d0fcab480a9f38ccc5ad1b8266a1f457\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/e8/68/f9ac36b8cc6c8b3c96888cd57434abed96595d444f42243853\n",
            "  Building wheel for sudachidict-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sudachidict-core: filename=SudachiDict_core-20230110-py3-none-any.whl size=71665396 sha256=50dc68e983594126e3ced00aeb275615ed3625657433344f7d1e61f9dcb9b268\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/35/8a/5cd8203a86e68ccefc49d4a2975165bb9ee369d2693eb4049b\n",
            "Successfully built sentence-transformers ipadic unidic-lite sudachidict-core\n",
            "Installing collected packages: wasabi, unidic-lite, tokenizers, sudachipy, sentencepiece, plac, ipadic, sudachidict-core, fugashi, huggingface-hub, transformers, spacy, ginza, ja_ginza, sentence-transformers\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.1\n",
            "    Uninstalling wasabi-1.1.1:\n",
            "      Successfully uninstalled wasabi-1.1.1\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.5.2\n",
            "    Uninstalling spacy-3.5.2:\n",
            "      Successfully uninstalled spacy-3.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 3.4.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fugashi-1.2.1 ginza-5.1.2 huggingface-hub-0.14.1 ipadic-1.0.0 ja_ginza-5.1.2 plac-1.3.5 sentence-transformers-2.2.2 sentencepiece-0.1.98 spacy-3.4.4 sudachidict-core-20230110 sudachipy-0.6.7 tokenizers-0.13.3 transformers-4.28.1 unidic-lite-1.0.8 wasabi-0.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git submodule update --init"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM28JBY3AaCs",
        "outputId": "55654c9c-e78d-416c-b051-3f6e8c48118a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submodule 'sbert-sts/JGLUE' (https://github.com/yahoojapan/JGLUE.git) registered for path 'JGLUE'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 sentence_bert_experiment.py --output-dir /content/drive/MyDrive/sbert-sts-model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EXtiknx7Vut",
        "outputId": "52b80e5f-9fe6-4a31-e083-d3277f556d2c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-01 06:31:37.413526: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-01 06:31:38.753386: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "GPU Memory: 15101.81 MB, batch_size=60\n",
            "Saved model with spearmanr: 0.6580 <- -inf\n",
            "[epoch: 0, iter: 0] loss: 0.2461, pearsonr: 0.6824, spearmanr: 0.6580\n",
            "[epoch: 0, iter: 10] loss: 0.0640, pearsonr: 0.6596, spearmanr: 0.6285\n",
            "Saved model with spearmanr: 0.7029 <- 0.6580\n",
            "[epoch: 0, iter: 20] loss: 0.0450, pearsonr: 0.7500, spearmanr: 0.7029\n",
            "Saved model with spearmanr: 0.7195 <- 0.7029\n",
            "[epoch: 0, iter: 30] loss: 0.0399, pearsonr: 0.7683, spearmanr: 0.7195\n",
            "Saved model with spearmanr: 0.7339 <- 0.7195\n",
            "[epoch: 0, iter: 40] loss: 0.0396, pearsonr: 0.7905, spearmanr: 0.7339\n",
            "Saved model with spearmanr: 0.7401 <- 0.7339\n",
            "[epoch: 0, iter: 50] loss: 0.0395, pearsonr: 0.8041, spearmanr: 0.7401\n",
            "Saved model with spearmanr: 0.7463 <- 0.7401\n",
            "[epoch: 0, iter: 60] loss: 0.0407, pearsonr: 0.8117, spearmanr: 0.7463\n",
            "Saved model with spearmanr: 0.7525 <- 0.7463\n",
            "[epoch: 0, iter: 70] loss: 0.0369, pearsonr: 0.8159, spearmanr: 0.7525\n",
            "Saved model with spearmanr: 0.7545 <- 0.7525\n",
            "[epoch: 0, iter: 80] loss: 0.0348, pearsonr: 0.8157, spearmanr: 0.7545\n",
            "Saved model with spearmanr: 0.7586 <- 0.7545\n",
            "[epoch: 0, iter: 90] loss: 0.0348, pearsonr: 0.8195, spearmanr: 0.7586\n",
            "Saved model with spearmanr: 0.7632 <- 0.7586\n",
            "[epoch: 0, iter: 100] loss: 0.0354, pearsonr: 0.8217, spearmanr: 0.7632\n",
            "Saved model with spearmanr: 0.7674 <- 0.7632\n",
            "[epoch: 0, iter: 110] loss: 0.0323, pearsonr: 0.8216, spearmanr: 0.7674\n",
            "Saved model with spearmanr: 0.7740 <- 0.7674\n",
            "[epoch: 0, iter: 120] loss: 0.0341, pearsonr: 0.8306, spearmanr: 0.7740\n",
            "Saved model with spearmanr: 0.7777 <- 0.7740\n",
            "[epoch: 0, iter: 130] loss: 0.0329, pearsonr: 0.8365, spearmanr: 0.7777\n",
            "Saved model with spearmanr: 0.7807 <- 0.7777\n",
            "[epoch: 0, iter: 140] loss: 0.0329, pearsonr: 0.8389, spearmanr: 0.7807\n",
            "Saved model with spearmanr: 0.7838 <- 0.7807\n",
            "[epoch: 0, iter: 150] loss: 0.0309, pearsonr: 0.8386, spearmanr: 0.7838\n",
            "Saved model with spearmanr: 0.7873 <- 0.7838\n",
            "[epoch: 0, iter: 160] loss: 0.0329, pearsonr: 0.8438, spearmanr: 0.7873\n",
            "Saved model with spearmanr: 0.7892 <- 0.7873\n",
            "[epoch: 1, iter: 0] loss: 0.0325, pearsonr: 0.8451, spearmanr: 0.7892\n",
            "[epoch: 1, iter: 10] loss: 0.0293, pearsonr: 0.8444, spearmanr: 0.7892\n",
            "Saved model with spearmanr: 0.7913 <- 0.7892\n",
            "[epoch: 1, iter: 20] loss: 0.0308, pearsonr: 0.8478, spearmanr: 0.7913\n",
            "Saved model with spearmanr: 0.7938 <- 0.7913\n",
            "[epoch: 1, iter: 30] loss: 0.0293, pearsonr: 0.8476, spearmanr: 0.7938\n",
            "Saved model with spearmanr: 0.7959 <- 0.7938\n",
            "[epoch: 1, iter: 40] loss: 0.0295, pearsonr: 0.8485, spearmanr: 0.7959\n",
            "Saved model with spearmanr: 0.7970 <- 0.7959\n",
            "[epoch: 1, iter: 50] loss: 0.0306, pearsonr: 0.8502, spearmanr: 0.7970\n",
            "[epoch: 1, iter: 60] loss: 0.0294, pearsonr: 0.8491, spearmanr: 0.7970\n",
            "Saved model with spearmanr: 0.7991 <- 0.7970\n",
            "[epoch: 1, iter: 70] loss: 0.0289, pearsonr: 0.8506, spearmanr: 0.7991\n",
            "Saved model with spearmanr: 0.8002 <- 0.7991\n",
            "[epoch: 1, iter: 80] loss: 0.0290, pearsonr: 0.8520, spearmanr: 0.8002\n",
            "[epoch: 1, iter: 90] loss: 0.0280, pearsonr: 0.8504, spearmanr: 0.7987\n",
            "[epoch: 1, iter: 100] loss: 0.0312, pearsonr: 0.8515, spearmanr: 0.7985\n",
            "Saved model with spearmanr: 0.8005 <- 0.8002\n",
            "[epoch: 1, iter: 110] loss: 0.0292, pearsonr: 0.8532, spearmanr: 0.8005\n",
            "[epoch: 1, iter: 120] loss: 0.0280, pearsonr: 0.8501, spearmanr: 0.8004\n",
            "Saved model with spearmanr: 0.8026 <- 0.8005\n",
            "[epoch: 1, iter: 130] loss: 0.0296, pearsonr: 0.8535, spearmanr: 0.8026\n",
            "Saved model with spearmanr: 0.8047 <- 0.8026\n",
            "[epoch: 1, iter: 140] loss: 0.0302, pearsonr: 0.8571, spearmanr: 0.8047\n",
            "Saved model with spearmanr: 0.8062 <- 0.8047\n",
            "[epoch: 1, iter: 150] loss: 0.0282, pearsonr: 0.8570, spearmanr: 0.8062\n",
            "[epoch: 1, iter: 160] loss: 0.0274, pearsonr: 0.8541, spearmanr: 0.8056\n",
            "[epoch: 2, iter: 0] loss: 0.0275, pearsonr: 0.8551, spearmanr: 0.8059\n",
            "Saved model with spearmanr: 0.8068 <- 0.8062\n",
            "[epoch: 2, iter: 10] loss: 0.0288, pearsonr: 0.8575, spearmanr: 0.8068\n",
            "Saved model with spearmanr: 0.8076 <- 0.8068\n",
            "[epoch: 2, iter: 20] loss: 0.0269, pearsonr: 0.8577, spearmanr: 0.8076\n",
            "[epoch: 2, iter: 30] loss: 0.0285, pearsonr: 0.8573, spearmanr: 0.8069\n",
            "Saved model with spearmanr: 0.8109 <- 0.8076\n",
            "[epoch: 2, iter: 40] loss: 0.0270, pearsonr: 0.8602, spearmanr: 0.8109\n",
            "Saved model with spearmanr: 0.8117 <- 0.8109\n",
            "[epoch: 2, iter: 50] loss: 0.0272, pearsonr: 0.8604, spearmanr: 0.8117\n",
            "[epoch: 2, iter: 60] loss: 0.0269, pearsonr: 0.8581, spearmanr: 0.8108\n",
            "[epoch: 2, iter: 70] loss: 0.0281, pearsonr: 0.8591, spearmanr: 0.8116\n",
            "Saved model with spearmanr: 0.8133 <- 0.8117\n",
            "[epoch: 2, iter: 80] loss: 0.0273, pearsonr: 0.8613, spearmanr: 0.8133\n",
            "Saved model with spearmanr: 0.8138 <- 0.8133\n",
            "[epoch: 2, iter: 90] loss: 0.0268, pearsonr: 0.8614, spearmanr: 0.8138\n",
            "[epoch: 2, iter: 100] loss: 0.0271, pearsonr: 0.8613, spearmanr: 0.8136\n",
            "[epoch: 2, iter: 110] loss: 0.0266, pearsonr: 0.8582, spearmanr: 0.8116\n",
            "[epoch: 2, iter: 120] loss: 0.0268, pearsonr: 0.8595, spearmanr: 0.8120\n",
            "[epoch: 2, iter: 130] loss: 0.0270, pearsonr: 0.8599, spearmanr: 0.8114\n",
            "[epoch: 2, iter: 140] loss: 0.0271, pearsonr: 0.8599, spearmanr: 0.8119\n",
            "[epoch: 2, iter: 150] loss: 0.0276, pearsonr: 0.8611, spearmanr: 0.8116\n",
            "[epoch: 2, iter: 160] loss: 0.0270, pearsonr: 0.8634, spearmanr: 0.8136\n",
            "Saved model with spearmanr: 0.8141 <- 0.8138\n",
            "[epoch: 3, iter: 0] loss: 0.0272, pearsonr: 0.8635, spearmanr: 0.8141\n",
            "Saved model with spearmanr: 0.8142 <- 0.8141\n",
            "[epoch: 3, iter: 10] loss: 0.0261, pearsonr: 0.8628, spearmanr: 0.8142\n",
            "[epoch: 3, iter: 20] loss: 0.0267, pearsonr: 0.8618, spearmanr: 0.8134\n",
            "[epoch: 3, iter: 30] loss: 0.0275, pearsonr: 0.8604, spearmanr: 0.8112\n",
            "[epoch: 3, iter: 40] loss: 0.0266, pearsonr: 0.8590, spearmanr: 0.8109\n",
            "[epoch: 3, iter: 50] loss: 0.0261, pearsonr: 0.8560, spearmanr: 0.8108\n",
            "Saved model with spearmanr: 0.8153 <- 0.8142\n",
            "[epoch: 3, iter: 60] loss: 0.0268, pearsonr: 0.8604, spearmanr: 0.8153\n",
            "Saved model with spearmanr: 0.8167 <- 0.8153\n",
            "[epoch: 3, iter: 70] loss: 0.0257, pearsonr: 0.8632, spearmanr: 0.8167\n",
            "Saved model with spearmanr: 0.8171 <- 0.8167\n",
            "[epoch: 3, iter: 80] loss: 0.0261, pearsonr: 0.8641, spearmanr: 0.8171\n",
            "[epoch: 3, iter: 90] loss: 0.0257, pearsonr: 0.8627, spearmanr: 0.8168\n",
            "[epoch: 3, iter: 100] loss: 0.0261, pearsonr: 0.8622, spearmanr: 0.8169\n",
            "Saved model with spearmanr: 0.8174 <- 0.8171\n",
            "[epoch: 3, iter: 110] loss: 0.0267, pearsonr: 0.8643, spearmanr: 0.8174\n",
            "[epoch: 3, iter: 120] loss: 0.0258, pearsonr: 0.8627, spearmanr: 0.8158\n",
            "Saved model with spearmanr: 0.8182 <- 0.8174\n",
            "[epoch: 3, iter: 130] loss: 0.0253, pearsonr: 0.8632, spearmanr: 0.8182\n",
            "Saved model with spearmanr: 0.8186 <- 0.8182\n",
            "[epoch: 3, iter: 140] loss: 0.0263, pearsonr: 0.8657, spearmanr: 0.8186\n",
            "[epoch: 3, iter: 150] loss: 0.0254, pearsonr: 0.8635, spearmanr: 0.8171\n",
            "[epoch: 3, iter: 160] loss: 0.0265, pearsonr: 0.8612, spearmanr: 0.8158\n",
            "[epoch: 4, iter: 0] loss: 0.0260, pearsonr: 0.8610, spearmanr: 0.8154\n",
            "[epoch: 4, iter: 10] loss: 0.0261, pearsonr: 0.8635, spearmanr: 0.8165\n",
            "Saved model with spearmanr: 0.8193 <- 0.8186\n",
            "[epoch: 4, iter: 20] loss: 0.0257, pearsonr: 0.8641, spearmanr: 0.8193\n",
            "Saved model with spearmanr: 0.8200 <- 0.8193\n",
            "[epoch: 4, iter: 30] loss: 0.0265, pearsonr: 0.8640, spearmanr: 0.8200\n",
            "Saved model with spearmanr: 0.8202 <- 0.8200\n",
            "[epoch: 4, iter: 40] loss: 0.0260, pearsonr: 0.8656, spearmanr: 0.8202\n",
            "[epoch: 4, iter: 50] loss: 0.0257, pearsonr: 0.8645, spearmanr: 0.8196\n",
            "[epoch: 4, iter: 60] loss: 0.0254, pearsonr: 0.8626, spearmanr: 0.8188\n",
            "[epoch: 4, iter: 70] loss: 0.0253, pearsonr: 0.8646, spearmanr: 0.8198\n",
            "Saved model with spearmanr: 0.8211 <- 0.8202\n",
            "[epoch: 4, iter: 80] loss: 0.0256, pearsonr: 0.8659, spearmanr: 0.8211\n",
            "[epoch: 4, iter: 90] loss: 0.0243, pearsonr: 0.8630, spearmanr: 0.8210\n",
            "[epoch: 4, iter: 100] loss: 0.0255, pearsonr: 0.8636, spearmanr: 0.8203\n",
            "[epoch: 4, iter: 110] loss: 0.0259, pearsonr: 0.8652, spearmanr: 0.8197\n",
            "[epoch: 4, iter: 120] loss: 0.0250, pearsonr: 0.8640, spearmanr: 0.8197\n",
            "[epoch: 4, iter: 130] loss: 0.0251, pearsonr: 0.8630, spearmanr: 0.8200\n",
            "[epoch: 4, iter: 140] loss: 0.0261, pearsonr: 0.8646, spearmanr: 0.8201\n",
            "[epoch: 4, iter: 150] loss: 0.0259, pearsonr: 0.8629, spearmanr: 0.8183\n",
            "[epoch: 4, iter: 160] loss: 0.0259, pearsonr: 0.8599, spearmanr: 0.8168\n",
            "[epoch: 5, iter: 0] loss: 0.0254, pearsonr: 0.8628, spearmanr: 0.8192\n",
            "Saved model with spearmanr: 0.8211 <- 0.8211\n",
            "[epoch: 5, iter: 10] loss: 0.0261, pearsonr: 0.8671, spearmanr: 0.8211\n",
            "[epoch: 5, iter: 20] loss: 0.0255, pearsonr: 0.8667, spearmanr: 0.8208\n",
            "[epoch: 5, iter: 30] loss: 0.0251, pearsonr: 0.8643, spearmanr: 0.8205\n",
            "[epoch: 5, iter: 40] loss: 0.0255, pearsonr: 0.8646, spearmanr: 0.8205\n",
            "[epoch: 5, iter: 50] loss: 0.0257, pearsonr: 0.8643, spearmanr: 0.8202\n",
            "[epoch: 5, iter: 60] loss: 0.0254, pearsonr: 0.8643, spearmanr: 0.8200\n",
            "[epoch: 5, iter: 70] loss: 0.0249, pearsonr: 0.8637, spearmanr: 0.8200\n",
            "[epoch: 5, iter: 80] loss: 0.0252, pearsonr: 0.8646, spearmanr: 0.8206\n",
            "Saved model with spearmanr: 0.8213 <- 0.8211\n",
            "[epoch: 5, iter: 90] loss: 0.0249, pearsonr: 0.8645, spearmanr: 0.8213\n",
            "Saved model with spearmanr: 0.8220 <- 0.8213\n",
            "[epoch: 5, iter: 100] loss: 0.0248, pearsonr: 0.8645, spearmanr: 0.8220\n",
            "Saved model with spearmanr: 0.8222 <- 0.8220\n",
            "[epoch: 5, iter: 110] loss: 0.0248, pearsonr: 0.8653, spearmanr: 0.8222\n",
            "[epoch: 5, iter: 120] loss: 0.0250, pearsonr: 0.8658, spearmanr: 0.8217\n",
            "[epoch: 5, iter: 130] loss: 0.0253, pearsonr: 0.8641, spearmanr: 0.8208\n",
            "[epoch: 5, iter: 140] loss: 0.0257, pearsonr: 0.8653, spearmanr: 0.8212\n",
            "[epoch: 5, iter: 150] loss: 0.0252, pearsonr: 0.8629, spearmanr: 0.8192\n",
            "[epoch: 5, iter: 160] loss: 0.0255, pearsonr: 0.8631, spearmanr: 0.8189\n",
            "[epoch: 6, iter: 0] loss: 0.0252, pearsonr: 0.8637, spearmanr: 0.8196\n",
            "[epoch: 6, iter: 10] loss: 0.0250, pearsonr: 0.8648, spearmanr: 0.8208\n",
            "[epoch: 6, iter: 20] loss: 0.0247, pearsonr: 0.8646, spearmanr: 0.8215\n",
            "[epoch: 6, iter: 30] loss: 0.0251, pearsonr: 0.8640, spearmanr: 0.8204\n",
            "[epoch: 6, iter: 40] loss: 0.0254, pearsonr: 0.8638, spearmanr: 0.8203\n",
            "[epoch: 6, iter: 50] loss: 0.0255, pearsonr: 0.8642, spearmanr: 0.8209\n",
            "[epoch: 6, iter: 60] loss: 0.0253, pearsonr: 0.8645, spearmanr: 0.8196\n",
            "[epoch: 6, iter: 70] loss: 0.0251, pearsonr: 0.8620, spearmanr: 0.8178\n",
            "[epoch: 6, iter: 80] loss: 0.0260, pearsonr: 0.8631, spearmanr: 0.8185\n",
            "[epoch: 6, iter: 90] loss: 0.0260, pearsonr: 0.8649, spearmanr: 0.8202\n",
            "[epoch: 6, iter: 100] loss: 0.0252, pearsonr: 0.8659, spearmanr: 0.8219\n",
            "[epoch: 6, iter: 110] loss: 0.0254, pearsonr: 0.8647, spearmanr: 0.8215\n",
            "[epoch: 6, iter: 120] loss: 0.0249, pearsonr: 0.8640, spearmanr: 0.8212\n",
            "Saved model with spearmanr: 0.8223 <- 0.8222\n",
            "[epoch: 6, iter: 130] loss: 0.0254, pearsonr: 0.8662, spearmanr: 0.8223\n",
            "[epoch: 6, iter: 140] loss: 0.0246, pearsonr: 0.8651, spearmanr: 0.8220\n",
            "[epoch: 6, iter: 150] loss: 0.0251, pearsonr: 0.8646, spearmanr: 0.8212\n",
            "Saved model with spearmanr: 0.8224 <- 0.8223\n",
            "[epoch: 6, iter: 160] loss: 0.0255, pearsonr: 0.8672, spearmanr: 0.8224\n",
            "[epoch: 7, iter: 0] loss: 0.0252, pearsonr: 0.8671, spearmanr: 0.8222\n",
            "[epoch: 7, iter: 10] loss: 0.0251, pearsonr: 0.8665, spearmanr: 0.8223\n",
            "Saved model with spearmanr: 0.8226 <- 0.8224\n",
            "[epoch: 7, iter: 20] loss: 0.0254, pearsonr: 0.8676, spearmanr: 0.8226\n",
            "[epoch: 7, iter: 30] loss: 0.0246, pearsonr: 0.8647, spearmanr: 0.8206\n",
            "[epoch: 7, iter: 40] loss: 0.0250, pearsonr: 0.8648, spearmanr: 0.8210\n",
            "[epoch: 7, iter: 50] loss: 0.0248, pearsonr: 0.8631, spearmanr: 0.8214\n",
            "[epoch: 7, iter: 60] loss: 0.0246, pearsonr: 0.8645, spearmanr: 0.8223\n",
            "Saved model with spearmanr: 0.8227 <- 0.8226\n",
            "[epoch: 7, iter: 70] loss: 0.0258, pearsonr: 0.8670, spearmanr: 0.8227\n",
            "Saved model with spearmanr: 0.8229 <- 0.8227\n",
            "[epoch: 7, iter: 80] loss: 0.0246, pearsonr: 0.8668, spearmanr: 0.8229\n",
            "[epoch: 7, iter: 90] loss: 0.0244, pearsonr: 0.8659, spearmanr: 0.8226\n",
            "[epoch: 7, iter: 100] loss: 0.0247, pearsonr: 0.8658, spearmanr: 0.8225\n",
            "[epoch: 7, iter: 110] loss: 0.0252, pearsonr: 0.8662, spearmanr: 0.8225\n",
            "[epoch: 7, iter: 120] loss: 0.0247, pearsonr: 0.8647, spearmanr: 0.8219\n",
            "[epoch: 7, iter: 130] loss: 0.0249, pearsonr: 0.8659, spearmanr: 0.8221\n",
            "[epoch: 7, iter: 140] loss: 0.0247, pearsonr: 0.8661, spearmanr: 0.8228\n",
            "[epoch: 7, iter: 150] loss: 0.0249, pearsonr: 0.8651, spearmanr: 0.8227\n",
            "[epoch: 7, iter: 160] loss: 0.0247, pearsonr: 0.8657, spearmanr: 0.8224\n",
            "[epoch: 8, iter: 0] loss: 0.0247, pearsonr: 0.8647, spearmanr: 0.8211\n",
            "[epoch: 8, iter: 10] loss: 0.0251, pearsonr: 0.8636, spearmanr: 0.8198\n",
            "[epoch: 8, iter: 20] loss: 0.0245, pearsonr: 0.8636, spearmanr: 0.8208\n",
            "Saved model with spearmanr: 0.8232 <- 0.8229\n",
            "[epoch: 8, iter: 30] loss: 0.0242, pearsonr: 0.8662, spearmanr: 0.8232\n",
            "Saved model with spearmanr: 0.8235 <- 0.8232\n",
            "[epoch: 8, iter: 40] loss: 0.0251, pearsonr: 0.8682, spearmanr: 0.8235\n",
            "[epoch: 8, iter: 50] loss: 0.0241, pearsonr: 0.8657, spearmanr: 0.8226\n",
            "[epoch: 8, iter: 60] loss: 0.0247, pearsonr: 0.8662, spearmanr: 0.8224\n",
            "[epoch: 8, iter: 70] loss: 0.0248, pearsonr: 0.8662, spearmanr: 0.8225\n",
            "[epoch: 8, iter: 80] loss: 0.0244, pearsonr: 0.8647, spearmanr: 0.8221\n",
            "[epoch: 8, iter: 90] loss: 0.0252, pearsonr: 0.8656, spearmanr: 0.8222\n",
            "[epoch: 8, iter: 100] loss: 0.0251, pearsonr: 0.8652, spearmanr: 0.8212\n",
            "[epoch: 8, iter: 110] loss: 0.0249, pearsonr: 0.8640, spearmanr: 0.8214\n",
            "[epoch: 8, iter: 120] loss: 0.0251, pearsonr: 0.8640, spearmanr: 0.8213\n",
            "[epoch: 8, iter: 130] loss: 0.0247, pearsonr: 0.8658, spearmanr: 0.8226\n",
            "[epoch: 8, iter: 140] loss: 0.0244, pearsonr: 0.8656, spearmanr: 0.8229\n",
            "[epoch: 8, iter: 150] loss: 0.0245, pearsonr: 0.8655, spearmanr: 0.8227\n",
            "[epoch: 8, iter: 160] loss: 0.0245, pearsonr: 0.8656, spearmanr: 0.8228\n",
            "[epoch: 9, iter: 0] loss: 0.0245, pearsonr: 0.8652, spearmanr: 0.8223\n",
            "Saved model with spearmanr: 0.8246 <- 0.8235\n",
            "[epoch: 9, iter: 10] loss: 0.0250, pearsonr: 0.8683, spearmanr: 0.8246\n",
            "Saved model with spearmanr: 0.8246 <- 0.8246\n",
            "[epoch: 9, iter: 20] loss: 0.0242, pearsonr: 0.8668, spearmanr: 0.8246\n",
            "[epoch: 9, iter: 30] loss: 0.0247, pearsonr: 0.8677, spearmanr: 0.8240\n",
            "[epoch: 9, iter: 40] loss: 0.0246, pearsonr: 0.8674, spearmanr: 0.8234\n",
            "[epoch: 9, iter: 50] loss: 0.0241, pearsonr: 0.8662, spearmanr: 0.8239\n",
            "Saved model with spearmanr: 0.8254 <- 0.8246\n",
            "[epoch: 9, iter: 60] loss: 0.0246, pearsonr: 0.8686, spearmanr: 0.8254\n",
            "Saved model with spearmanr: 0.8257 <- 0.8254\n",
            "[epoch: 9, iter: 70] loss: 0.0250, pearsonr: 0.8692, spearmanr: 0.8257\n",
            "Saved model with spearmanr: 0.8264 <- 0.8257\n",
            "[epoch: 9, iter: 80] loss: 0.0241, pearsonr: 0.8683, spearmanr: 0.8264\n",
            "[epoch: 9, iter: 90] loss: 0.0241, pearsonr: 0.8676, spearmanr: 0.8250\n",
            "[epoch: 9, iter: 100] loss: 0.0248, pearsonr: 0.8691, spearmanr: 0.8250\n",
            "[epoch: 9, iter: 110] loss: 0.0244, pearsonr: 0.8673, spearmanr: 0.8236\n",
            "[epoch: 9, iter: 120] loss: 0.0242, pearsonr: 0.8647, spearmanr: 0.8222\n",
            "[epoch: 9, iter: 130] loss: 0.0246, pearsonr: 0.8650, spearmanr: 0.8217\n",
            "[epoch: 9, iter: 140] loss: 0.0248, pearsonr: 0.8660, spearmanr: 0.8227\n",
            "[epoch: 9, iter: 150] loss: 0.0243, pearsonr: 0.8652, spearmanr: 0.8231\n",
            "[epoch: 9, iter: 160] loss: 0.0243, pearsonr: 0.8668, spearmanr: 0.8245\n",
            "[Best model] loss: 0.0264, pearsonr: 0.8499, spearmanr: 0.7954\n",
            "実行時間: 4431 秒\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EgOilgwl7s5-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}